# app.py
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from langchain.prompts import PromptTemplate
import json

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ====== Globales ======
qa_reglamento = None
qa_all = None
qa_enfasis = None
qa_electivas = None
qa_complementarias = None


@app.on_event("startup")
def load_rag():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    # ========= REGLAMENTO =========
    vectorstore_reglamento = FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )

    prompt_reglamento = PromptTemplate.from_template(
        """Responde la siguiente pregunta en espa√±ol de forma clara y precisa usando la informaci√≥n disponible:

{context}

Pregunta: {question}

Respuesta:"""
    )

    llm = Ollama(model="llama3", temperature=0.3)

    global qa_reglamento
    qa_reglamento = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore_reglamento.as_retriever(),
        chain_type_kwargs={"prompt": prompt_reglamento},
        input_key="question",
    )

    # ========= MATERIAS (4 √≠ndices) =========
    def load_index(path: str):
        return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

    vector_all = load_index("faiss_materias")               # TODAS (por defecto)
    vector_enfasis = load_index("faiss_enfasis")            # √ânfasis
    vector_electivas = load_index("faiss_electivas")        # Electivas
    vector_complementarias = load_index("faiss_complementarias")  # Complementarias

    prompt_materias = PromptTemplate.from_template(
        """
Eres un sistema de recomendaci√≥n de materias universitarias. 
Debes responder **√∫nicamente en formato JSON v√°lido**.

üìå Reglas estrictas:
- SOLO utiliza materias presentes en el CONTEXTO.
- Si el estudiante indica un n√∫mero espec√≠fico de cr√©ditos, SOLO devuelve materias que tengan exactamente esos cr√©ditos (usa el valor literal tal como aparece).
- Si el estudiante elige "Cualquiera" en cr√©ditos, ignora ese filtro y recomienda solo en base a intereses.
- NO inventes ni cambies los valores de cr√©ditos, ID, cat√°logo ni oferta. Copia exactamente lo que aparezca en el contexto.
- Incluye una justificaci√≥n clara: afinidad entre los intereses del estudiante y el contenido/competencias de la materia.
- Si no hay coincidencias exactas, devuelve "materias": [] y una explicaci√≥n clara.

üìå Formato de salida obligatorio:
{{
  "materias": [
    {{
      "nombre": "...",
      "grado": "...",
      "id": "...",
      "creditos": "...",
      "numero_catalogo": "...",
      "numero_oferta": "...",
      "razon": "Explica brevemente por qu√© esta materia fue recomendada seg√∫n los intereses del estudiante."
    }}
  ],
  "explicacion": "Explicaci√≥n general de la recomendaci√≥n o por qu√© no se encontraron resultados."
}}

üìå Contexto:
{context}

üìå Consulta del estudiante:
{question}
"""
    )

    global qa_all, qa_enfasis, qa_electivas, qa_complementarias
    qa_all = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector_all.as_retriever(search_kwargs={"k": 10}),
        chain_type_kwargs={"prompt": prompt_materias},
        input_key="question",
    )
    qa_enfasis = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector_enfasis.as_retriever(search_kwargs={"k": 10}),
        chain_type_kwargs={"prompt": prompt_materias},
        input_key="question",
    )
    qa_electivas = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector_electivas.as_retriever(search_kwargs={"k": 10}),
        chain_type_kwargs={"prompt": prompt_materias},
        input_key="question",
    )
    qa_complementarias = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector_complementarias.as_retriever(search_kwargs={"k": 10}),
        chain_type_kwargs={"prompt": prompt_materias},
        input_key="question",
    )

    print("‚úÖ Servicios RAG cargados correctamente")


# ========= REGLAMENTO =========
@app.post("/query")
async def query(request: Request):
    data = await request.json()
    question = data.get("question", "")
    result = qa_reglamento.invoke({"question": question})
    return {"answer": result["result"]}


# ========= RECOMENDACI√ìN (elige √≠ndice por tipo) =========
@app.post("/recomendar-materias")
async def recomendar_materias(request: Request):
    data = await request.json()
    intereses = (data.get("intereses") or "").strip()
    creditos = data.get("creditos", None)
    tipo = (data.get("tipo") or "cualquiera").strip().lower()

    # Construcci√≥n de consulta
    if creditos and str(creditos).lower() != "cualquiera":
        consulta = f"Intereses del estudiante: {intereses}. SOLO devolver materias con Cr√©ditos: {creditos}."
    else:
        consulta = f"Intereses del estudiante: {intereses}. No aplicar filtro de cr√©ditos."

    # Elegir QA por tipo
    qa = {
        "cualquiera": qa_all,
        "√©nfasis": qa_enfasis,
        "enfasis": qa_enfasis,
        "electivas": qa_electivas,
        "complementarias": qa_complementarias,
    }.get(tipo, qa_all)

    result = qa.invoke({"question": consulta})
    raw = result["result"]

    print(f"üîç Tipo seleccionado: {tipo}")
    print("üîç Respuesta cruda del modelo:")
    print(raw)

    try:
        parsed = json.loads(raw)
        return parsed
    except json.JSONDecodeError:
        return {
            "materias": [],
            "explicacion": "No se pudo interpretar correctamente la recomendaci√≥n generada."
        }
